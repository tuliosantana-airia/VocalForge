from pathlib import Path
from typing import Optional

import torch
from datasets import Dataset
from pyannote.audio import Pipeline
from tqdm import tqdm

from .audio_utils import export_from_timestamps, get_files, get_timestamps


class Overlap:
    def __init__(self, input_dir: str, output_dir: str, hparams: Optional[dict] = None):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.input_files = get_files(self.input_dir, True, ".wav")
        self.timelines = []
        self.ds = Dataset.from_dict({"audio": self.input_files})
        self.hparams = hparams

        # Create a pipeline object using the pre-trained "pyannote/overlapped-speech-detection"
        self.pipeline = Pipeline.from_pretrained(
            "pyannote/overlapped-speech-detection", use_auth_token=True
        )

        self.is_hparams = False
        if self.hparams is not None:
            self.pipeline.instantiate(self.hparams)
            self.is_hparams = True

        self.pipeline.to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))

    def analyze(self) -> list:
        """
        Analyzes audio files for overlapping speech.

        This function processes each audio file in the dataset, applying an
        overlapping speech detection pipeline to detect periods of overlapping
        speech. It collects these detected periods as timelines and appends them
        to the class's timelines attribute. It also updates the dataset by adding
        a new column containing the timestamps of detected overlapping speech for
        each file.
        """
        timestamps = []
        for example in tqdm(
            self.ds, total=len(self.ds), desc="Analyzing files"
        ):
            overlap_timeline = self.pipeline(example["audio"])
            self.timelines.append(overlap_timeline)
            timestamps.append(get_timestamps(overlap_timeline))

        self.ds = self.ds.add_column("timestamps", timestamps)

    def export(self):
        """
        Exports overlapping speech segments based on the overlapping speech timestamps
        generated by the analyze method.

        This function processes each audio file in the dataset, exporting the overlapping
        speech segments as new wav files in the specified output directory.
        """
        for example in tqdm(
            self.ds,
            total=len(self.ds),
            desc="Exporting Speech Segments",
        ):
            base_file_name = Path(example["audio"]).name
            export_from_timestamps(
                example["audio"],
                self.output_dir / base_file_name,
                example["timestamps"],
                combine_mode="time_between",
            )

    def run(self):
        """runs the overlap detection pipeline"""
        if list(self.input_dir.glob("*")) != []:
            self.analyze()
            self.export()

        print("Analyzed files for voice detection")
