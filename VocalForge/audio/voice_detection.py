from pathlib import Path
from typing import Optional

import torch
from datasets import Dataset
from pyannote.audio import Pipeline
from tqdm import tqdm

from .audio_utils import export_from_timestamps, get_files, get_timestamps


class VoiceDetection:
    def __init__(
        self,
        input_dir: str,
        output_dir: str,
        sample_dir: Optional[str] = None,
        hparams: Optional[dict] = None,
    ):
        """
        Initializes a new instance of the VoiceDetection class.

        Parameters:
            input_dir (str): The directory containing the input audio files to analyze.
            output_dir (str): The directory where the output audio files will be saved.
            sample_dir (str): The directory containing sample audio files to analyze.
        """
        if sample_dir is not None:
            self.input_dir = Path(sample_dir)
        else:
            self.input_dir = Path(input_dir)
            self.output_dir = Path(output_dir)
        self.input_files = get_files(str(self.input_dir), True, ".wav")
        self.ds = Dataset.from_dict({"audio": self.input_files})
        self.timelines = []
        self.hparams = hparams

        self.pipeline = Pipeline.from_pretrained(
            "pyannote/voice-activity-detection", use_auth_token=True
        )

        # instantiate the pipeline with hyperparameters if declared
        self.is_hparams = False
        if self.hparams is not None:
            self.pipeline.instantiate(self.hparams)
            self.is_hparams = True

        self.pipeline.to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))

    def analyze(self):
        """
        Analyzes voice activity in audio files, generating speech activity timestamps.

        This function processes each audio file in the dataset, applying a voice
        activity detection pipeline to detect periods of speech. It collects these
        detected periods as timelines and appends them to the class's timelines attribute.
        It also updates the dataset by adding a new column containing the timestamps
        of detected speech activity for each file.
        """
        timestamps = []
        for example in tqdm(self.ds, total=len(self.ds), desc="Analyzing files"):
            timeline = self.pipeline(example["audio"])
            self.timelines.append(timeline)
            timestamps.append(get_timestamps(timeline))

        self.ds = self.ds.add_column("timestamps", timestamps)

    def export(self):
        """
        Exports speech segments based on the speech activity timestamps generated by the analyze method.

        This function processes each audio file in the dataset, exporting the speech segments
        as new wav files in the specified output directory.
        """
        for example in tqdm(
            self.ds,
            total=len(self.ds),
            desc="Exporting Speech Segments",
        ):
            base_file_name = Path(example["audio"]).name
            export_from_timestamps(
                example["audio"],
                str(self.output_dir / base_file_name),
                example["timestamps"],
            )

    def run(self):
        """runs the voice detection pipeline"""
        if list(self.input_dir.glob("*")) != []:
            self.analyze()
            self.export()

        print("Analyzed files for voice detection")
